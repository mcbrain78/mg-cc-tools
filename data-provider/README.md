# Data Provider Field Mapping

Adversarial research pipeline that maps financial data fields to provider APIs.
Two AI agents (researcher + verifier) independently evaluate each field-provider
combination to produce verified API mappings.

For design rationale, architectural decisions, and known limitations see [DESIGN.md](DESIGN.md).

## Installation

Use the install script to set up the command and supporting files. It copies
everything into place and resolves internal file paths to absolute paths so
the LLM can find the scripts at runtime.

### Project-level (current project only)

```bash
cd /path/to/your/project
/path/to/mg-cc-tools/data-provider/install.sh --project
```

This installs into `<project>/.claude/` and scaffolds `.mg/data-provider/`.

### Global (all projects)

```bash
./install.sh --global
```

This installs into `~/.claude/`. You'll need to create `.mg/data-provider/`
manually in each project.

### Custom target

```bash
./install.sh --target /path/to/your/.claude
```

### What the installer does

1. Copies command file to `<target>/commands/mg/map-fields-research.md`
2. Copies Python scripts to `<target>/data-provider/scripts/`
3. Copies reference files to `<target>/data-provider/references/`
4. Copies DESIGN.md to `<target>/data-provider/`
5. Resolves `{SCRIPTS_DIR}` placeholder to absolute path in the command file
6. (`--project` only) Creates `.mg/data-provider/` with input/tasks/output dirs
   and seeds default field reference + provider list

### Installed structure

```
<target>/                               (your .claude/ directory)
├── commands/mg/
│   └── map-fields-research.md          ← orchestrator command
└── data-provider/
    ├── DESIGN.md                       ← design rationale
    ├── scripts/
    │   ├── generate.py                 ← create task files from inputs
    │   ├── status.py                   ← list/read/update task state
    │   └── summarize.py                ← generate coverage report
    └── references/
        └── 00-field-reference.md       ← template field definitions

<project>/                              (--project mode only)
└── .mg/data-provider/
    ├── README.md                       ← usage guide
    ├── DESIGN.md                       ← design rationale
    ├── input/
    │   ├── 00-field-reference.md       ← field definitions (edit for your use case)
    │   └── providers.txt               ← provider names, one per line
    ├── tasks/                          ← task files (one per field×provider)
    └── output/
        └── coverage-report.md          ← generated by summarizer
```

### Dependencies

- **Required:** `python3` 3.10+ (for the script type hints)
- **No pip dependencies.** Scripts use only the standard library.

---

## Usage

### 1. Prepare inputs

Edit `.mg/data-provider/input/providers.txt` with your provider names (one per line).
Edit `.mg/data-provider/input/00-field-reference.md` if your field definitions differ.

### 2. Generate task files

```bash
python <scripts-path>/generate.py
```

Creates one task file per (field, provider) pair in `.mg/data-provider/tasks/`.
Safe to re-run — skips existing files.

### 3. Run adversarial research

```
/mg:map-fields-research
```

The command asks for model and parallelism settings, then processes all pending
tasks through the researcher+verifier loop.

### 4. Generate coverage report

```bash
python <scripts-path>/summarize.py
```

Produces `.mg/data-provider/output/coverage-report.md` with:
- Coverage matrix (field x provider)
- Per-field detail tables with endpoint, JSON path, and docs URL
- Inconclusive items needing manual review

---

## How It Works

1. **Generate** creates task files from the field reference and provider list
2. **Research** spawns agent pairs per task:
   - Researcher finds the API endpoint, JSON path, and documentation URL
   - Verifier independently fetches the same docs and confirms the claim
   - If rejected: researcher retries once with feedback. Second failure = inconclusive.
3. **Summarize** reads all task files and produces the coverage report

Match types: **DIRECT** (field available as-is), **DERIVABLE** (computable from
raw inputs), **NONE** (not available). No substitutes — binary outcomes only.

### Task file lifecycle

```
pending → researched → verified     (researcher found it, verifier confirmed)
pending → researched → pending      (rejected, retry with feedback)
pending → researched → inconclusive (rejected twice, needs manual review)
```

To retry an inconclusive task:
```bash
python <scripts-path>/status.py update --file <filename> --status pending
python <scripts-path>/status.py clear-research --file <filename>
```

### Editing inputs

- Add/remove providers: edit `input/providers.txt`, re-run generate (existing tasks preserved)
- Change field definitions: edit `input/00-field-reference.md`, delete affected tasks, re-run generate
